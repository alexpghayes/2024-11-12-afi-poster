\PassOptionsToPackage{dvipsnames}{xcolor}
\documentclass[final]{beamer}

\usepackage[T1]{fontenc}
\usepackage{lmodern}

% set poster dimensions below (in cm)
% AFI / the posterboards at the Discovery Building are 46"x46".
% 46" wide x 36" high (121.92 x 91.44 cm)
% A0 for ASA WI Chapter, which is slightly smaller 84.1 x 118.9
\usepackage[size=custom,width=118.9,height=84.1, scale=1]{beamerposter}
\usetheme{gemini}
\usecolortheme{mit}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{nicematrix}

\usepackage[backend=bibtex, sortcites, style=authoryear]{biblatex}
\addbibresource{references.bib}

% https://tex.stackexchange.com/questions/585635/beamer-biblatex-authoryear-causes-problem-with-insertbiblabel
\setbeamertemplate{bibliography item}{}


\usepackage{tikz}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{anyfontsize}
\usepackage{hyperref}

\usepackage{hayesmacros}
\usepackage{amsmath}
\usepackage{amsthm}

\newtheorem{proposition}{Proposition}

% Define Dark Red 2 color (RGB: 139,0,0)
% \definecolor{Mahogany}{RGB}{139,0,0}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{Estimating peer influence: limitations of linear-in-means models}

\author{Alex Hayes \inst{1} \and Keith Levin \inst{1}}

\institute[shortinst]{\inst{1} Department of Statistics, University of Wisconsin-Madison}

% ====================
% Footer (optional)
% ====================

% \footercontent{\hfill \href{https://www.alexpghayes.com}{https://www.alexpghayes.com}}
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
\logoright{\includegraphics[height=7cm]{figures/logos/color-flush-UWlogo-print.pdf}}

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
    \begin{columns}[t]
        \separatorcolumn

        \begin{column}{\colwidth}

            % \begin{alertblock}{Abstract}

            %     Linear-in-means models are widely used to investigate peer effects. Identifying peer effects in these models is challenging, but conditions for identification are well-known. However, even when peer effects are identified, they may not be estimable, due to an asymptotic colinearity issue: as sample size increases, peer effects become more and more linearly dependent.  We show that asymptotic colinearity occurs whenever nodal covariates are independent of the network and the minimum degree of the network is growing. Asymptotic colinearity can cause estimators to be inconsistent or to converge at slower than expected rates. We also demonstrate that dependence between nodal covariates and network structure can alleviate colinearity issues in random dot product graphs. These results suggest that linear-in-means models are less reliable for studying peer influence than previously believed.

            % \end{alertblock}

            \begin{block}{Understanding social influence is a fundamental problem in a highly connected society}

                \begin{columns}
                    \column{0.3\textwidth}
                    \centering
                    \includegraphics[width=0.9\textwidth]{./figures/assortative.png}
                    \column{0.7\textwidth}
                    \textbf{Contagion}: if my friends get sick, I am more likely to get sick \\
                    \vspace{8mm}
                    \textbf{Direct effect}: if I get vaccinated, I am less likely to get sick \\
                    \vspace{8mm}
                    \textbf{Interference}: if my friends get vaccinated, I am less likely to get sick \\
                \end{columns}

            \end{block}

            \begin{block}{The linear-in-means model is a canonical tool to estimate peer influence in network data}

                % Given a network with adjacency matrix $A \in \{0, 1\}^{n \times n}$, and outcomes and treatments for each node, the model assume outcomes depend on treatment and also peer influence from neighboring nodes. Let $\Ni = \{j \in [n]: A_{ij} = 1\}$ be the set of neighbors of node $i$.

                % \textbf{Intuition}: imagine $Y_i \in \{0, 1\}$. Then:

                % Suppose there is a network with $n$ nodes, encoded by a symmetric adjacency matrix $A \in \R^{n \times n}$. In binary networks, $A_{ij} = 1$ if nodes $i$ and $j$ form an edge, and $A_{ij} = 0$ otherwise, though we note that our results do not require that $A$ be binary. Each node $i$ is associated with an outcome $Y_i \in \R$ and a covariate $T_i \in \R$. Letting $\Ni = \left\{ j \in [n]: A_{ij} = 1 \right\}$ denote the neighbors of node $i$ in the network, the treatment and outcome of the neighbors are allowed to influence the outcome of node $i$ as follows:
                \begin{equation*}
                    \underbrace{Y_i}_\text{sick?} =
                    \alpha +
                    \beta \underbrace{\frac{1}{d_i}\sum_{j \in \Ni} Y_j}_{\substack{\text{fraction} \\ \text{sick} \\ \text{friends}}} +
                    \gamma \, T_i +
                    \delta \underbrace{\frac{1}{d_i}\sum_{j \in \Ni} T_j}_{\substack{\text{fraction} \\ \text{vaccinated} \\ \text{friends}}} +
                    \varepsilon_i
                \end{equation*}

                \begin{minipage}{.5\textwidth}
                    \textbf{Data:}
                    \vspace{3mm}
                    \begin{table}[]
                        \begin{tabular}{llcl}
                            Outcome         & (sick?)       & $Y_i$    & $\in \{0, 1\}$             \\
                            Treatment       & (vaccinated?) & $T_i$    & $\in \{0, 1\}$             \\
                            Edge $i \sim j$ & (friends?)    & $A_{ij}$ & $\in \{0, 1\}$             \\
                            Degree          & (num friends) & $d_i$    & $\in \set{0, 1, 2, \dots}$
                        \end{tabular}
                    \end{table}
                \end{minipage}
                \begin{minipage}{.5\textwidth}
                    \textbf{Parameters:}
                    \vspace{3mm}
                    \begin{table}[]
                        \begin{tabular}{lcl}
                            Base rate     & $\alpha$ & $\in \R$      \\
                            Contagion     & $\beta$  & $\in (-1, 1)$ \\
                            Direct effect & $\gamma$ & $\in \R$      \\
                            Interference  & $\delta$ & $\in \R$
                        \end{tabular}
                    \end{table}
                \end{minipage}

                Letting $G = D^{-1} A$ be the row-normalized adjacency matrix, can express in matrix-vector form:
                \begin{equation*} \label{eq:lim-mv}
                    Y = \alpha 1_n + \beta G Y + T \gamma + G T \delta + \varepsilon
                \end{equation*}
            \end{block}

            \begin{block}{Linear-in-means models are famously suspectible to the ``reflection problem,'' an identification failure due to colinearity}

                In highly structured networks, the peer effect terms can be perfectly colinear, such that peer effects cannot be estimated from the data. For example, in a fully connected network:

                \begin{figure}
                    \begin{minipage}{0.49\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \node[shape=circle,fill=Mahogany,label=above left:{$T_1 = 1$}] (A) at (0,1) {};
                            \node[shape=circle,fill=Mahogany,label=below left:{$T_2 = 0$}] (B) at (1,0) {};
                            \node[shape=circle,fill=Mahogany,label=above right:{$T_3 = 1$}] (C) at (1.5,1.5) {};
                            \node[shape=circle,fill=Mahogany,label=above right:{$T_4 = 1$}] (D) at (2.75,0.5) {};

                            \path (A) edge [loop above] node {} (A);
                            \path (B) edge [loop below] node {} (B);
                            \path (C) edge [loop above] node {} (C);
                            \path (D) edge [loop above] node {} (D);

                            \draw (A) -- (B);
                            \draw (A) -- (C);
                            \draw (A) -- (D);
                            \draw (B) -- (C);
                            \draw (B) -- (D);
                            \draw (C) -- (D);
                        \end{tikzpicture}
                    \end{minipage}
                    \begin{minipage}{0.49\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \node[shape=circle,fill=Mahogany,label=above left:{$[GT]_1 = 3/4$}] (A) at (0,1) {};
                            \node[shape=circle,fill=Mahogany,label=below left:{$[GT]_2 = 3/4$}] (B) at (1,0) {};
                            \node[shape=circle,fill=Mahogany,label=above right:{$[GT]_3 = 3/4$}] (C) at (1.5,1.5) {};
                            \node[shape=circle,fill=Mahogany,label=above right:{$[GT]_4 = 3/4$}] (D) at (2.75,0.5) {};

                            \path (A) edge [loop above] node {} (A);
                            \path (B) edge [loop below] node {} (B);
                            \path (C) edge [loop above] node {} (C);
                            \path (D) edge [loop above] node {} (D);

                            \draw (A) -- (B);
                            \draw (A) -- (C);
                            \draw (A) -- (D);
                            \draw (B) -- (C);
                            \draw (B) -- (D);
                            \draw (C) -- (D);
                        \end{tikzpicture}
                    \end{minipage}
                \end{figure}

                \begin{equation*}
                    \begin{bmatrix}
                        Y_1 \\
                        Y_2 \\
                        Y_3 \\
                        Y_4
                    \end{bmatrix}
                    =
                    \begin{bNiceMatrix}[first-row,first-col]
                         &                         &      &   &                           \\
                         & \textcolor{Mahogany}{1} & GY_1 & 1 & \textcolor{Mahogany}{3/4} \\
                         & \textcolor{Mahogany}{1} & GY_2 & 0 & \textcolor{Mahogany}{3/4} \\
                         & \textcolor{Mahogany}{1} & GY_3 & 1 & \textcolor{Mahogany}{3/4} \\
                         & \textcolor{Mahogany}{1} & GY_4 & 1 & \textcolor{Mahogany}{3/4} \\
                    \end{bNiceMatrix}
                    \begin{bmatrix}
                        \textcolor{Mahogany}{\alpha} \\
                        \beta                        \\
                        \gamma                       \\
                        \textcolor{Mahogany}{\delta}
                    \end{bmatrix}
                    +
                    \begin{bmatrix}
                        \varepsilon_1 \\
                        \varepsilon_2 \\
                        \varepsilon_3 \\
                        \varepsilon_4
                    \end{bmatrix}
                \end{equation*}

                Can't distinguish base rate $\textcolor{Mahogany}{\alpha}$ from interference $\textcolor{Mahogany}{\delta}$ due to colinearity
            \end{block}



            \begin{block}{It's widely believed that colinearity problems can be avoided when certain identifying conditions, such as intransitivity, hold}

                \textbf{Proposition:}
                Suppose $\mathbb E[\varepsilon \mid T] = 0$ and that $\abs{\beta} < 1$ and $\gamma \beta + \delta \neq 0$. For any fixed $n$, if $I, G$ and $G^2$ are linearly independent (i.e., $a I + b G + c G^2 = 0$ only if $a = b = c = 0$), then $\alpha, \beta, \gamma$ and $\delta$ are identified. If $I, G$ and $G^2$ are linearly dependent and no node is isolated, then $(\alpha, \beta, \gamma, \delta)$ are not identified.

                \textbf{Intuition}: Peer influence is identified when there are \textbf{open triangles} (``intransitivity'') in the network
                \vspace{8mm}
                \begin{columns}
                    \begin{column}{0.5\textwidth}
                        \centering
                        \begin{tikzpicture}
                            \node[shape=circle,fill=Mahogany,label=above left:A] (A) at (0,1) {};
                            \node[shape=circle,fill=Mahogany,label=below left:B] (B) at (1,0) {};
                            \node[shape=circle,fill=Mahogany,label=above right:C] (C) at (1.5,1.5) {};
                            \node[shape=circle,fill=Mahogany,label=above right:D] (D) at (2.75,0.5) {};
                            \draw (A) -- (B);
                            \draw (A) -- (C);
                            \draw (B) -- (C);
                            \draw (C) -- (D);
                        \end{tikzpicture}
                    \end{column}
                    \begin{column}{0.5\textwidth}
                        \centering
                        % \underline{i.e., 1+ open triangle identifies peer effects} \\
                        % \vspace{4mm}
                        Closed: \textcolor{Mahogany}{$A \leftrightarrow B \leftrightarrow C \leftrightarrow A$} \\
                        Open: $B \leftrightarrow C \leftrightarrow D \nleftrightarrow B$
                    \end{column}
                \end{columns}
            \end{block}

        \end{column}

        \separatorcolumn

        \begin{column}{\colwidth}


            \begin{block}{We show that peer effects can be inestimable even when identifying conditions hold}
                Three popular estimators all fail to estimate parameters at expected $\sqrt{n}$ rates in simulations, even though half of all possible triangles in the network are open
                \begin{figure}
                    \centering
                    \includegraphics[width=0.8\textwidth]{./figures/simulations/biometrika-mse.pdf}
                \end{figure}
            \end{block}

            \begin{block}{The problem is that peer effect terms are getting more and more colinear as the number of nodes in the network increases}

                \textbf{Intuition:} Imagine vaccination is a coin flip for every node (i.e., a Bernoulli design). As the network grows, the fraction of vaccinated friends $\approx 0.5$ for every single node.

                \begin{equation} \label{eq:avg}
                    \lim_{d_i \to \infty}
                    \underbrace{[GT]_i}_{\substack{\text{fraction} \\ \text{vaccinated} \\ \text{friends}}}
                    =
                    \lim_{d_i \to \infty}
                    \underbrace{
                        \frac{1}{d_i} \sum_{j \in \Ni} T_j
                    }_{\substack{\text{average of $d_i$}           \\ \text{i.i.d. coin flips}}}
                    = \frac 12
                \end{equation}
                This also causes the contagion term to be near-colinear with the intercept, since $GY$ is a repeated diffusion of $T$ and $GT$ over the network
                \begin{align*}
                    Y & = \alpha 1_n + \beta G Y + \gamma T + \delta G T + \varepsilon                            \\
                      & = \paren*{I - \beta G}^{-1} \paren*{\alpha 1_n + \gamma T + \delta G T + \varepsilon}     \\
                      & = \sum_{k=0}^\infty \beta^k G^k \paren*{\alpha 1_n + \gamma T + \delta G T + \varepsilon}
                \end{align*}

                \textbf{Lemma:} Suppose that (1) the nodal covariates $T_1, T_2, \dots$ are independent with shared mean $\tau \in \R$, and $T$ is independent of $A$; (2) the nodal covariates are sub-gamma random variables; (3) the regression errors $\varepsilon_1, \varepsilon_2, \dots$ are independent subgamma random variables independent of $T$.

                If the minimum degree of the network grows at a $\omega(\log n)$ rate, then there exists $\eta \in \R$ such that
                \begin{equation*}
                    \max_{i \in [n]} \Big| [GT]_i - \tau \Big|
                    = o(1) ~ \text{ and }
                    \max_{i \in [n]} \Big| [GY]_i - \eta \Big| = o(1)~ \text{ almost surely.}
                \end{equation*}
            \end{block}

            \begin{block}{Asymptotic colinearity causes leads to inconsistency because the signal-to-noise ratio gets worse and worse with growing sample size}

                \textbf{Theorem:} Under the same conditions as the Lemma, let $(\alphahat, \betahat, \gammahat, \deltahat)$ be the vector of ordinary least squares estimates of $(\alpha, \beta, \gamma, \delta)$. Suppose that the degrees of the network are such that $\| G \|_F^2 = o(n)$.
                Then if $\beta = 0$,
                \begin{equation*}
                    \min\{ |\alphahat-\alpha|, |\betahat-\beta| \}
                    = \Omegap{ 1 }
                \end{equation*}
                and
                \begin{equation} \label{eq:deltahat:LB}
                    | \deltahat - \delta | = \Omegap{ \frac{1}{\|G\|_F} }.
                \end{equation}
                If $\beta \neq 0$,
                \begin{equation*}
                    \min\{ |\alphahat-\alpha|, |\betahat-\beta| \}
                    = \Omegap{ \frac{1}{\|G\|_F} }.
                \end{equation*}
                Under the stronger growth assumption $\|G\|_F^2 = o( \sqrt{n} )$, eq.~\eqref{eq:deltahat:LB} holds for all values of $\beta$.
                \vspace{3mm}

                \textbf{Intuition:} In networks with growing minimum degree, ordinary least squares estimates of $\alpha, \beta$ and $\delta$ are either inconsistent, or at best consistent at $\sqrt{n / d_\mathrm{min}}$ rates, where $d_\mathrm{min} = \min_{i \in [n]} d_i$.
            \end{block}
        \end{column}

        \separatorcolumn

        \begin{column}{\colwidth}
            \begin{block}{When nodal covariates are strongly associated with network structure, it is sometimes possible to avoid asymptotic colinearity}

                \textbf{Intuition:} the fraction of vaccinated peers in eq. \eqref{eq:avg} might not converge to a column of constants if treatment $T$ depends highly on position in the network $A$

                \textbf{Random dot product graphs:} Suppose $X_1, ..., X_n \in \R^d$ are i.i.d. from a distribution $F$ such that $0 \le x^T y < 1$ for all $x,y \in \supp F$. Then $\mathbb{P}(A_{ij} \mid X_i, X_j) = X_i^T X_j$.

                \textbf{Theorem:} Suppose that $(A, X)$ are sampled from a random dot product model where $X \in \mathbb{R}^{n \times d}$ is full-rank with high probability. Let
                \begin{equation} \label{eq:lim-rdpg}
                    Y = \alpha 1_n + \beta G Y + X \gamma + G X \delta + \varepsilon
                \end{equation}
                for $\alpha, \beta \in \R$ and $\gamma, \delta \in \R^d$. Suppose that $X$ has $k \ge 2d$ distinct rows. Then, under suitable technical conditions, the columns of design matrix corresponding to $(\alpha, \beta, \delta_1, \delta_2, \dots, \delta_d)$ are asymptotically colinear. If any two elements of $(\alpha, \beta, \delta_1, \delta_2, \dots, \delta_d)$ are equal to zero, there is no asymptotic colinearity.

                \textbf{Simulation:} In a network generated according to eq. \eqref{eq:lim-rdpg} with no coefficients set to zero (\emph{Unrestricted} model), there are still colinearity and estimation issues. However, when two coefficients from $(\alpha, \beta, \delta_1, \delta_2, \dots, \delta_d)$ set to zero (\emph{Restricted} model), popular estimators recover regression coefficients at expected $\sqrt{n}$ rates.

                \begin{figure}
                    \centering
                    \includegraphics[width=\textwidth]{./figures/simulations/biometrika-mse-all.pdf}
                    \caption{\textcolor{Mahogany}{Red lines} represent estimation error for asymptotically aliased regression coefficients, and \textcolor{gray}{gray lines} represent estimation error for asymptotic un-aliased coefficients.}
                \end{figure}

            \end{block}

            \begin{block}{Takeaways}
                \begin{enumerate}
                    \item It can be impossible to estimate peer effects using linear-in-means models, even when all parameters in the model are identified. This is primarily an issue for Bernoulli designs in dense networks, or under models with growing minimum degree.
                    \item In observational data from random dot product models (like stochastic blockmodels), it may be possible to avoid colinearity issues by including latent network structure in the regression
                    \item In controlled experiments, use network-specific designs, like graph-cluster or ego-cluster randomization, to avoid the colinearity issues of Bernoulli designs
                \end{enumerate}
            \end{block}

            \begin{block}{Want to learn more? Have a comment? Pre-print \& contact info}
                \nocite{hayes2024c}
                \printbibliography
                \begin{center}
                    \url{alex.hayes@wisc.edu} \\
                    \url{https://www.alexpghayes.com}
                \end{center}
            \end{block}


        \end{column}

        \separatorcolumn
    \end{columns}
\end{frame}



% The coefficient $\beta$, typically called the ``contagion term'', measures how peer outcomes $Y_j$ influence the outcome $Y_i$ at vertex $i$.
% This is variously referred to elsewhere in the literature as an ``exogeneous spatial lag'' \, a ``spatial autoregression'' or an ``endogeneous peer effect'' .
% Similarly, the coefficient $\delta$, typically called the ``interference term'', measures how peer treatments $T_j$ influence $i$'s outcome $Y_i$.
% Elsewhere in the literature, $\delta$ is variously referred to as a ``contextual peer effect'', an ``exogeneous peer effect'', a ``spatial Durbin term,'' or a ``spatially lagged X'' term.

% The linear-in-means model has been the subject of considerable attention due to challenges identifying the peer effects \citep[see][for a recent review]{bramoulle2020}. \cite{manski1993} famously showed that peer effects are not identified in highly structured social networks. Much of the difficulty comes from the presence of $Y$ on both sides of of equation~\eqref{eq:lim-avg}.
% Manski's result was subsequently generalized by Proposition~\ref{prop:bramoulle2009} of \cite{bramoulle2009}, which showed that identification of the linear-in-means model is possible, provided that there are some open triangles (``intransivity'') or some variation in group sizes in the network. Since most social networks feature open triangles or variation in group sizes, the linear-in-means model is generally understood to be identified.


% In the linear-in-means model, each outcome $Y_i$ is a function of all the other outcomes $Y_1, Y_2, \dots, Y_{i-1}, Y_{i+1}, \dots , Y_n$. This simultaneity \citep[sometimes known as the ``reflection problem'', see][]{manski1993} makes it challenging to understand the data generating process of the linear-in-means model. In this section, we introduce the reduced form of the linear-in-means model, explain the generative process, and describe conditions for identification. Understanding identification in the linear-in-means provides insight into asymptotic colinearity and why it might pose a problem for estimation. In essence, asymptotic colinearity corresponds to identification getting weaker and weaker with increasing sample size.

% We begin by expressing the linear-in-means model in matrix-vector form. Define the degree matrix $D = \diag(d_1, d_2, \dots, d_n)$, where $d_i = \sum_j A_{ij}$. Let $G = D^{-1} A$ be the row-normalized adjacency matrix. Multiplication by $G$ thus maps nodal values to the neighborhood averages of these values, in the sense that $[GY]_i = d_i^{-1} \sum_j A_{ij} Y_j$ (see Fig.~\ref{fig:averaging} for an illustration). Incorporating this notation, we have
% \begin{equation} \label{eq:lim-mv}
%     Y = \alpha 1_n + \beta G Y + T \gamma + G T \delta + \varepsilon,
% \end{equation}
% where $GY \in \R^n$ encodes the vector of neighborhood averages of $Y$ and $GT \in \R^n$ encodes the neighborhood averages of $T$. Thus, the design matrix of the linear-in-means model is
% \begin{equation} \label{eq:def:W}
%     W_n = \begin{bmatrix} 1_n & GY & T & GT \end{bmatrix}.
% \end{equation}

% We assume, as is typical, that the errors $\varepsilon$ are mean-zero and independent of the network $A$ and nodal covariates $T$. We don't make parametric assumptions on the noise terms $\varepsilon$. Later on, we will consider multiple nodal covariates, allowing $T_i \in \R^p$ to be vector-valued. Thus, $\gamma, \delta \in \R^p$ are allowed to be vector-valued.

% To work with equation~\eqref{eq:lim-mv}, we solve for $Y$ and consider the reduced form specification of the model, which is possible provided that $I - \beta G$ is invertible (i.e., when $\abs{\beta} < 1$). Employing the Neumann expansion $\paren*{I - \beta G}^{-1} = \sum_{k=0}^\infty \beta^k G^k$, we can write
% \begin{equation} \label{eq:lim-red}
%     Y = \paren*{I - \beta G}^{-1} \paren*{1_n \alpha + T \gamma + G T \delta + \varepsilon}      \\
%     = \sum_{k=0}^\infty \beta^k G^k \paren*{1_n \alpha + T \gamma + G T \delta + \varepsilon}.
% \end{equation}
% This reduced form describes how outcomes $Y$ can be generated given a network $A$, nodal covariates $T$ and errors $\varepsilon$. The expression further suggests that $Y$ can be interpreted as the equilibrium state reached after repeated neighborhood averaging on the network. That is, one  way to sample an outcome vector $Y$ is to construct an initial outcome vector $Y^{(0)} = \alpha 1_n + \gamma T + \delta G T + \varepsilon$, and then to repeatedly diffuse this outcome over the network according to $G$, weighting by $\beta$ each time. To be very concrete: once $Y^{(0)}$ has been sampled, compute the scaled neighborhood average value $\beta G (\alpha 1_n + \gamma T + \delta G T + \varepsilon)$ and add it to $Y^{(0)}$ to construct a new outcome vector $Y^{(1)} = \beta G Y^{(0)} + Y^{(0)}$. Then compute the average neighborhood value of $Y^{(2)} = \beta G Y^{(1)} + Y^{(1)}$, $Y^{(3)}$, and so on. Repeating this process infinitely many times produces the equilibrium value $Y$ in equation~\eqref{eq:lim-red}, which is guaranteed to be finite and unique by the fact that $\abs*{\beta} < 1$.

% Interpreting the regression coefficients $(\alpha, \beta, \gamma, \delta)$ in light of the repeated diffusion process for $Y$ can be challenging, as the typical interpretation of linear regression coefficients no longer applies. \cite{vazquez-bare2023} and \citet[][Chapter 2]{lesage2009} discuss these considerations in detail. Similarly, estimating the regression coefficients requires specialized techniques, as the contagion and interference terms introduce dependence between entries of the outcome vector $Y$. Some approaches to estimation are given by \cite{ord1975, kelejian2001,lee2002,lee2003,lee2004,kelejian2007,lee2010, su2012, drukker2013, lin2010a} and surveyed in \cite{bivand2021}.

\end{document}
